{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fonttools Pillow==9.1.1 torch torchmetrics torchmetrics[image]\n",
    "!pip install -U torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env OMP_NUM_THREADS=8\n",
    "%env MKL_NUM_THREADS=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wj6MEwvs1Bg9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(24)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from fontTools.ttLib import TTFont\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageChops\n",
    "from PIL.Image import Resampling\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_7giZw51sm1"
   },
   "outputs": [],
   "source": [
    "def get_glyph_image(glyph, font_name, glyph_size, img_size):\n",
    "    font = ImageFont.truetype(f\"fonts-src/{font_name}.otf\", glyph_size - glyph_size // 10)\n",
    "\n",
    "    image = Image.new(\"L\", (img_size, img_size), \"white\")\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    offset_w, offset_h = font.getoffset(glyph)\n",
    "    w, h = draw.textsize(glyph, font = font)\n",
    "    pos = ((img_size - w - offset_w) / 2, (img_size - h - offset_h) / 2)\n",
    "\n",
    "    draw.text(pos, glyph, \"black\", font = font)\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_glyph_PNG(image, glyph_name, font_name, output_folder):\n",
    "    if (not os.path.exists(f\"{output_folder}\")):\n",
    "        os.mkdir(f\"{output_folder}\")\n",
    "    \n",
    "    if (not os.path.exists(f\"{output_folder}/{font_name}\")):\n",
    "        os.mkdir(f\"{output_folder}/{font_name}\")\n",
    "    \n",
    "    image.save(f\"{output_folder}/{font_name}/{glyph_name}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_chars_from_font(font_name):\n",
    "    with TTFont(f\"fonts-src/{font_name}.otf\") as font:\n",
    "        characters = []\n",
    "        for t in font[\"cmap\"].tables:\n",
    "            if (not t.isUnicode()):\n",
    "                continue\n",
    "            \n",
    "            for c in t.cmap.items():\n",
    "                characters.append((str(chr(c[0])), c[1]))\n",
    "                \n",
    "        return set(characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glyph_size(glyph, font_name, img_size):\n",
    "    image = Image.new(\"L\", (img_size, img_size), \"white\")\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    l, r = 1, img_size * 4\n",
    "    while (l + 1 < r):\n",
    "        m = (l + r) // 2\n",
    "        \n",
    "        re_font = ImageFont.truetype(f\"fonts-src/{font_name}.otf\", m)\n",
    "        it_font = ImageFont.truetype(f\"fonts-src/{font_name}i.otf\", m)\n",
    "        \n",
    "        re_w, re_h = draw.textsize(glyph, font = re_font)\n",
    "        it_w, it_h = draw.textsize(glyph, font = it_font)\n",
    "        \n",
    "        if (re_w > img_size or re_h > img_size or it_w > img_size or it_h > img_size):\n",
    "            r = m\n",
    "        else:\n",
    "            l = m\n",
    "    \n",
    "    return l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_glyph_equality(glyph, font_name, img_size):\n",
    "    re_font = ImageFont.truetype(f\"fonts-src/{font_name}.otf\", img_size // 2)\n",
    "    it_font = ImageFont.truetype(f\"fonts-src/{font_name}i.otf\", img_size // 2)\n",
    "\n",
    "    re_image = Image.new(\"L\", (img_size, img_size), \"white\")\n",
    "    re_draw = ImageDraw.Draw(re_image)\n",
    "    it_image = Image.new(\"L\", (img_size, img_size), \"white\")\n",
    "    it_draw = ImageDraw.Draw(it_image)\n",
    "\n",
    "    re_draw.text((img_size // 8, img_size // 8), glyph, \"black\", font = re_font)\n",
    "    it_draw.text((img_size // 8, img_size // 8), glyph, \"black\", font = it_font)\n",
    "\n",
    "    diff = ImageChops.difference(re_image, it_image)\n",
    "    return (diff.getbbox() is None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oucpi4p88ybq"
   },
   "outputs": [],
   "source": [
    "def draw_font_set_PNG(font_name, re_output, it_output, img_size):\n",
    "    re_chars = get_all_chars_from_font(font_name)\n",
    "    it_chars = get_all_chars_from_font(font_name + \"i\")\n",
    "    \n",
    "    chars = re_chars.intersection(it_chars)\n",
    "    \n",
    "    for glyph, glyph_name in chars:\n",
    "        if (glyph.isspace()):\n",
    "            continue\n",
    "\n",
    "        if (glyph_name == \".null\"):\n",
    "            continue\n",
    "            \n",
    "        if (ord(glyph[0]) > 0x2116):\n",
    "            continue\n",
    "            \n",
    "        glyph_size = get_glyph_size(glyph, font_name, img_size)\n",
    "\n",
    "        re_img = get_glyph_image(glyph, font_name, glyph_size, img_size)\n",
    "        it_img = get_glyph_image(glyph, font_name + \"i\", glyph_size, img_size)\n",
    "        \n",
    "        draw_glyph_PNG(re_img, glyph_name, font_name, re_output)\n",
    "        draw_glyph_PNG(it_img, glyph_name, font_name + \"i\", it_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDKJTDSpZMWH"
   },
   "outputs": [],
   "source": [
    "def draw_fonts_PNG(re_output, it_output, img_size):\n",
    "    if (os.path.exists(re_output)):\n",
    "        shutil.rmtree(re_output)\n",
    "        os.mkdir(re_output)\n",
    "        \n",
    "    if (os.path.exists(it_output)):\n",
    "        shutil.rmtree(it_output)\n",
    "        os.mkdir(it_output)\n",
    "    \n",
    "    re_fonts = sorted(list(filter(lambda f: \"i\" not in f, os.listdir(\"fonts-src\"))))\n",
    "    it_fonts = sorted(list(filter(lambda f: \"i\" in f, os.listdir(\"fonts-src\"))))\n",
    "    assert(re_fonts == list(map(lambda s: s.replace(\"i\", \"\"), it_fonts)))\n",
    "    \n",
    "    for font_name in tqdm(re_fonts):\n",
    "        draw_font_set_PNG(font_name.replace(\".otf\", \"\"), re_output, it_output, img_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6TBHQIxHZccx",
    "outputId": "0514bbe9-4268-42b2-d006-acce33923f81",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 128\n",
    "CHANNELS_CNT = 1\n",
    "\n",
    "def draw_all_fonts(img_size):\n",
    "    return\n",
    "    \n",
    "    draw_fonts_PNG(f\"fonts-re-{img_size}\", f\"fonts-it-{img_size}\", img_size)\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "draw_all_fonts(img_size = IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0,1,2,3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as tr\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import OrderedDict\n",
    "from enum import Enum\n",
    "import gc\n",
    "\n",
    "torch.set_num_threads(8)\n",
    "\n",
    "RE_FONTS_PATH = f\"fonts-re-{IMAGE_SIZE}/\"\n",
    "IT_FONTS_PATH = f\"fonts-it-{IMAGE_SIZE}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gpu():\n",
    "    #return\n",
    "    \n",
    "    model.to(\"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split():\n",
    "    np.random.seed(24)\n",
    "    \n",
    "    fonts = list(filter(lambda f: \"i\" not in f, os.listdir(\"fonts-src\")))\n",
    "    val_cnt = len(fonts) // 15\n",
    "    test_cnt = len(fonts) // 15\n",
    "    \n",
    "    fonts = np.array(fonts)\n",
    "    np.random.shuffle(fonts)\n",
    "    return list(fonts[: -(val_cnt + test_cnt)]), list(fonts[-(val_cnt + test_cnt) : -test_cnt]), list(fonts[-test_cnt :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_glyphs(fonts):\n",
    "    re_glyphs, it_glyphs = [], []\n",
    "    for font in fonts:\n",
    "        re = sorted(os.listdir(f\"{RE_FONTS_PATH}/{font.replace('.otf', '')}\"))\n",
    "        it = sorted(os.listdir(f\"{IT_FONTS_PATH}/{font.replace('.otf', '')}i\"))\n",
    "        assert(re == it)\n",
    "        \n",
    "        for glyph in re:\n",
    "            img = Image.open(f\"{RE_FONTS_PATH}/{font.replace('.otf', '')}/{glyph}\")\n",
    "            re_glyphs.append(np.array(img))\n",
    "            \n",
    "        for glyph in it:\n",
    "            img = Image.open(f\"{IT_FONTS_PATH}/{font.replace('.otf', '')}i/{glyph}\")\n",
    "            it_glyphs.append(np.array(img))\n",
    "            \n",
    "    return re_glyphs, it_glyphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = tr.Compose([\n",
    "    tr.ToTensor(),\n",
    "    #tr.Normalize(channel_mean, channel_std),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mode(Enum):\n",
    "    train = 0\n",
    "    test = 1\n",
    "    val = 2\n",
    "\n",
    "class GlyphDataset(Dataset):\n",
    "    def __init__(self, re, it, mode):\n",
    "        assert(len(re) == len(it))\n",
    "        self.re = re\n",
    "        self.it = it\n",
    "        \n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.re)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        re = transforms(self.re[index])\n",
    "        it = transforms(self.it[index])\n",
    "        \n",
    "        return re, it\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fonts, val_fonts, test_fonts = train_val_test_split()\n",
    "print(len(train_fonts), len(val_fonts), len(test_fonts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:3\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_ds = GlyphDataset(*retrieve_glyphs(train_fonts), Mode.train)\n",
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    drop_last = True,\n",
    "    num_workers = 0\n",
    ")\n",
    "\n",
    "val_ds = GlyphDataset(*retrieve_glyphs(val_fonts), Mode.val)\n",
    "val_dl = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = False,\n",
    "    drop_last = False,\n",
    "    num_workers = 0\n",
    ")\n",
    "\n",
    "test_ds = GlyphDataset(*retrieve_glyphs(test_fonts), Mode.test)\n",
    "test_dl = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = False,\n",
    "    drop_last = False,\n",
    "    num_workers = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def correct_picture(pic, lower_bound = 0.0, upper_bound = 0.8):\n",
    "    res = deepcopy(pic)\n",
    "    \n",
    "    for i in range(res.shape[0]):\n",
    "        for j in range(res.shape[1]):\n",
    "            if (res[i][j] > upper_bound):\n",
    "                res[i][j] = 1\n",
    "            elif (res[i][j] < lower_bound):\n",
    "                res[i][j] = 0\n",
    "            elif (lower_bound == upper_bound):\n",
    "                res[i][j] = 0\n",
    "            else:\n",
    "                res[i][j] = (res[i][j] - lower_bound) / (upper_bound - lower_bound)\n",
    "    \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import MeanSquaredError, MeanAbsoluteError, StructuralSimilarityIndexMeasure\n",
    "from torchmetrics.classification import BinaryAccuracy, BinaryF1Score\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def train(model, optimizer, loader, criterion):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    for re, it in tqdm(loader):\n",
    "        re = re.to(device)\n",
    "        it = it.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out = model(re)\n",
    "        loss = criterion(out, it)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item()) \n",
    "    \n",
    "    return model, optimizer, np.mean(losses)\n",
    "\n",
    "\n",
    "def val(model, loader, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    mse = MeanSquaredError().to(device)\n",
    "    mae = MeanAbsoluteError().to(device)\n",
    "    acc = BinaryAccuracy().to(device)\n",
    "    fsc = BinaryF1Score().to(device)\n",
    "    ssim = StructuralSimilarityIndexMeasure().to(device)\n",
    "    \n",
    "    loss_l, mse_l, mae_l, acc_l, fsc_l, ssim_l = [], [], [], [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for re, it in tqdm(loader):\n",
    "            re = re.to(device)\n",
    "            it = it.to(device)\n",
    "            \n",
    "            out = model(re)\n",
    "            \n",
    "            loss_l.append(criterion(out, it).item())\n",
    "            \n",
    "            mse_l.append(mse(out, it).item())\n",
    "            mae_l.append(mae(out, it).item())\n",
    "            \n",
    "            correcred_it = it.round().type(torch.int)\n",
    "            \n",
    "            acc_l.append(acc(out, correcred_it).item())\n",
    "            fsc_l.append(fsc(out, correcred_it).item())\n",
    "            \n",
    "            ssim_l.append(ssim(out, it).item())\n",
    "    \n",
    "    loss = np.mean(loss_l)\n",
    "    mse = np.mean(mse_l)\n",
    "    mae = np.mean(mae_l)\n",
    "    acc = np.mean(acc_l)\n",
    "    fsc = np.mean(fsc_l)\n",
    "    ssim = np.mean(ssim_l)\n",
    "    \n",
    "    print(\n",
    "        f\"loss: {'{:.6f}'.format(loss)}; \" + \\\n",
    "        f\"mse: {'{:.6f}'.format(mse)}; \" + \\\n",
    "        f\"mae: {'{:.6f}'.format(mae)}; \" + \\\n",
    "        f\"acc: {'{:.3f}'.format(acc)}; \" + \\\n",
    "        f\"fsc: {'{:.3f}'.format(fsc)}; \" + \\\n",
    "        f\"ssim: {'{:.3f}'.format(ssim)} \",\n",
    "        end = \"\\n\\n\", flush = True\n",
    "    )\n",
    "    \n",
    "    return loss, mse, mae, acc, fsc, ssim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_loop(\n",
    "    model, optimizer,\n",
    "    train_loader, val_loader,\n",
    "    criterion, epochs = 10,\n",
    "    scheduler = None, min_lr = None,\n",
    "    val_every = 1, draw_every = 1\n",
    "):\n",
    "    metrics = {\"train\": [], \"val\": []}\n",
    "\n",
    "    best_loss = 2.0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"#{epoch}/{epochs}:\", flush = True)\n",
    "        \n",
    "        model, optimizer, loss = train(model, optimizer, train_loader, criterion)\n",
    "        metrics[\"train\"].append(loss)\n",
    "\n",
    "        if (epoch % val_every == 0):\n",
    "            val_metrics = val(model, val_loader, criterion)\n",
    "            metrics[\"val\"].append(val_metrics)\n",
    "            val_loss = val_metrics[0]\n",
    "            \n",
    "            if scheduler:\n",
    "                scheduler.step(val_loss)\n",
    "                \n",
    "            if (val_loss < best_loss):\n",
    "                best_loss = val_loss\n",
    "\n",
    "                torch.save({\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict()\n",
    "                }, \"./model\")\n",
    "    \n",
    "    return model, optimizer, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_and_optimizer(model_class, model_params, lr = 1e-3, device = device):\n",
    "    model = model_class(**model_params)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    params = []\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            params.append(param)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(params, lr)\n",
    "    return model, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_net(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, hidden_sizes, activations, dropouts, output_size):\n",
    "        super(simple_net, self).__init__()\n",
    "        \n",
    "        flat = (\"flat\", nn.Flatten())\n",
    "        in_to_hid = (\"in2hid\", nn.Linear(input_size, hidden_sizes))\n",
    "        \n",
    "        head = [\n",
    "            (f\"act_last\", nn.ReLU()),\n",
    "            (\"hid2out\", nn.Linear(hidden_sizes, output_size)),\n",
    "            (\"sigmoid\", nn.Sigmoid())\n",
    "        ]\n",
    "        \n",
    "        self.net = [flat, in_to_hid, *head]\n",
    "        self.net = nn.Sequential(OrderedDict(self.net))\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        return torch.reshape(self.net(inp), (-1, CHANNELS_CNT, IMAGE_SIZE, IMAGE_SIZE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        stride = 2 if (in_channels != out_channels) else 1\n",
    "\n",
    "        if (in_channels != out_channels):\n",
    "            self.shortcut = nn.Sequential(OrderedDict([\n",
    "                (\"downsample_conv\", nn.Conv2d(\n",
    "                    in_channels, out_channels,\n",
    "                    kernel_size = 1, stride = 2,\n",
    "                    bias = False\n",
    "                )),\n",
    "                (\"downsample_norm\", nn.BatchNorm2d(out_channels))\n",
    "            ]))\n",
    "        \n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, out_channels,\n",
    "            kernel_size = 3, stride = stride,\n",
    "            padding = 1, dilation = 1,\n",
    "            groups = 1, bias = False\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels, out_channels,\n",
    "            kernel_size = 3, stride = 1,\n",
    "            padding = 1, dilation = 1,\n",
    "            groups = 1, bias = False\n",
    "        )\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.stride = stride\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.shortcut(x)\n",
    "        out = x\n",
    "\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.activation(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        out += residual\n",
    "        out = self.activation(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNetLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.Sequential(OrderedDict([\n",
    "            (\"residual1\", ResidualBlock(in_channels, out_channels)),\n",
    "            (\"residual2\", ResidualBlock(out_channels, out_channels))\n",
    "        ]))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.blocks(x)\n",
    "\n",
    "\n",
    "class MyResNet64(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_size = 64\n",
    "        self.channels_cnt = 1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            self.channels_cnt, 64,\n",
    "            kernel_size = 3, stride = 1,\n",
    "            padding = 1, dilation = 1,\n",
    "            groups = 1, bias = False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(\n",
    "            kernel_size = 3, stride = 2,\n",
    "            padding = 1, dilation = 1\n",
    "        )\n",
    "\n",
    "        self.layers = nn.Sequential(OrderedDict([\n",
    "            #(\"resnet1\", ResNetLayer(64, 64)),\n",
    "            (\"resnet2\", ResNetLayer(64, 128)),\n",
    "            (\"resnet3\", ResNetLayer(128, 256)),\n",
    "            (\"resnet4\", ResNetLayer(256, 512))\n",
    "        ]))\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(in_features = 512 * 4 * 4, out_features = self.image_size ** 2, bias = True)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        out = self.layers(out)\n",
    "\n",
    "        out = self.flatten(out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return torch.reshape(out, (-1, self.channels_cnt, self.image_size, self.image_size))\n",
    "\n",
    "\n",
    "class MyResNet128(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_size = 128\n",
    "        self.channels_cnt = 1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            self.channels_cnt, 64,\n",
    "            kernel_size = 3, stride = 1,\n",
    "            padding = 1, dilation = 1,\n",
    "            groups = 1, bias = False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(\n",
    "            kernel_size = 3, stride = 2,\n",
    "            padding = 1, dilation = 1\n",
    "        )\n",
    "\n",
    "        self.layers = nn.Sequential(OrderedDict([\n",
    "            (\"resnet1\", ResNetLayer(64, 128)),\n",
    "            (\"resnet2\", ResNetLayer(128, 256)),\n",
    "            (\"resnet3\", ResNetLayer(256, 512))\n",
    "        ]))\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size = 2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(in_features = 512 * 4 * 4, out_features = self.image_size ** 2, bias = True)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        out = self.layers(out)\n",
    "\n",
    "        out = self.avgpool(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return torch.reshape(out, (-1, self.channels_cnt, self.image_size, self.image_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "'''\n",
    "model, optimizer = create_model_and_optimizer(\n",
    "    simple_net,\n",
    "    {\n",
    "        \"input_size\": IMAGE_SIZE ** 2,\n",
    "        \"num_layers\": 0,\n",
    "        \"hidden_sizes\": 2 ** 12,\n",
    "        \"activations\": 0,\n",
    "        \"dropouts\": 0,\n",
    "        \"output_size\": IMAGE_SIZE ** 2\n",
    "    },\n",
    "    lr = 1e-4\n",
    ")\n",
    "'''\n",
    "\n",
    "model, optimizer = create_model_and_optimizer(\n",
    "    MyResNet128,\n",
    "    {},\n",
    "    lr = 1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model, optimizer, losses = learning_loop(model, optimizer, train_dl, val_dl, criterion, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_params_cnt(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"./model\", map_location = device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(get_model_params_cnt(model))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(model, loader):\n",
    "    np.random.seed(24)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (re, it) in enumerate(loader):\n",
    "            re = re.to(device)\n",
    "            it = it.to(device)\n",
    "            \n",
    "            index = np.random.randint(0, re.shape[0])\n",
    "            re = re[index].unsqueeze(0)\n",
    "            it = it[index].unsqueeze(0)\n",
    "            \n",
    "            out = model(re)\n",
    "            \n",
    "            ini_pic = re.detach().cpu().squeeze().numpy()\n",
    "            \n",
    "            res_pic = correct_picture(out.detach().cpu().squeeze().numpy())\n",
    "            \n",
    "            # resampling\n",
    "            # res_pic = Image.fromarray(np.uint8(res_pic))\n",
    "            # width, height = res_pic.width, res_pic.height\n",
    "            # res_pic = res_pic.resize((width * 2, height * 2))\n",
    "            # res_pic = res_pic.resize((width, height), resample = Resampling.BOX)\n",
    "            # res_pic = np.array(res_pic)\n",
    "            \n",
    "            tgt_pic = it.detach().cpu().squeeze().numpy()\n",
    "            \n",
    "            _, axarr = plt.subplots(1, 3)\n",
    "            axarr[0].imshow(ini_pic, cmap = \"gray\", interpolation = None)\n",
    "            axarr[1].imshow(res_pic, cmap = \"gray\", interpolation = None)\n",
    "            axarr[2].imshow(tgt_pic, cmap = \"gray\", interpolation = None)\n",
    "            plt.show()\n",
    "            \n",
    "            res_pic = Image.fromarray(np.uint8(res_pic * 255), mode = \"L\")\n",
    "            res_pic.save(f\"samples-png-in/{i}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_results(model, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
